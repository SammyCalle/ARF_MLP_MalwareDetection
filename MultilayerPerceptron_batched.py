#!/bin/env python
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
from sklearn.model_selection import train_test_split
from sklearn.metrics import precision_score,recall_score,accuracy_score,f1_score

real_legitimate = pd.read_csv('real_legitimate_v1.csv')
real_malware = pd.read_csv('real_malware_v1.csv')

def initial_clean(real_legitimate, real_malware):
    i = 1980
    year = str(i) + "-"
    removelist = real_legitimate.query('HighestModDate.str.contains(@year)', engine='python')
    removelistMalware = real_malware.query('HighestModDate.str.contains(@year)', engine='python')
    real_legitimate = real_legitimate.drop(index=removelist.index)
    real_malware = real_malware.drop(index=removelistMalware.index)

    real_legitimate['HighestModDate'] = pd.to_datetime(real_legitimate['HighestModDate'] )
    real_malware['HighestModDate'] = pd.to_datetime(real_malware['HighestModDate'] )

    return real_legitimate, real_malware

real_legitimate , real_malware = initial_clean(real_legitimate=real_legitimate, real_malware=real_malware)

sys_calls = []
permission = []
counter = 1
for i in real_legitimate.columns:
    if(counter > 2 and counter < 291):
        sys_calls.append(i)
    if(counter > 291 and counter < 458):
        permission.append(i)
    counter = counter + 1

def percentage (part , whole):
    percentage = 100 * float(part)/float(whole)
    return "%.2f" % round(percentage,2)

drops = ['Package','MalFamily','nr_permissions','normal','dangerous','signature','custom_yes','nr_custom'
        ,'total_perm','sha256','CFileSize','UFileSize','EarliestModDate'
,'Detection_Ratio','nr_syscalls','Scanners','FilesInsideAPK','TimesSubmitted','NrContactedIps']
listStr = ['Activities','NrIntServices','NrIntServicesActions','NrIntActivities','NrIntActivitiesActions','NrIntReceivers'
,'NrIntReceiversActions','TotalIntentFilters','NrServices']

def date_constraining(initial_date,final_date,real_legitimate,real_malware,splitter= False):
#Initial dat = "2011-01-01", "2018-12-31"
#splliter 3MS = 3 meses
    real_legitimate = real_legitimate.loc[real_legitimate["HighestModDate"].between(initial_date,final_date)]
    real_malware = real_malware.loc[real_malware["HighestModDate"].between(initial_date,final_date)]

    data = pd.concat([real_legitimate, real_malware], ignore_index=True)

    data = data.drop(drops,axis= 1)
    data = data.drop(listStr,axis= 1)
    data = data.drop(permission,axis= 1)
    data = data.sample(frac = 1)

    if splitter == False :
        return data
    else :
        treeMonth = data.groupby(pd.Grouper(key='HighestModDate', freq=splitter))
        dfstreeMonth = [group for _,group in treeMonth]
        return dfstreeMonth

monthList3 = ["2011Q1","2011Q2","2011Q3","2011Q4"
            ,"2012Q1","2012Q2","2012Q3","2012Q4"
            ,"2013Q1","2013Q2","2013Q3","2013Q4"
            ,"2014Q1","2014Q2","2014Q3","2014Q4"
            ,"2015Q1","2015Q2","2015Q3","2015Q4"
            ,"2016Q1","2016Q2","2016Q3","2016Q4"
            ,"2017Q1","2017Q2","2017Q3","2017Q4"
            ,"2018Q1","2018Q2","2018Q3","2018Q4"]


LEARNING_RATE = 0.001
NUM_EPOCHS = 200

MLPmodel = nn.Sequential(
    nn.Linear(in_features=288, out_features=52),
    nn.ReLU(),
    nn.Linear(in_features=52, out_features=64),
    nn.ReLU(),
    nn.Linear(in_features=64, out_features=128),
    nn.ReLU(),
    nn.Linear(in_features=128, out_features=64),
    nn.ReLU(),
    nn.Linear(in_features=64, out_features=16),
    nn.ReLU(),
    nn.Linear(in_features=16, out_features=8),
    nn.ReLU(),
    nn.Linear(in_features= 8, out_features=1),
    nn.Sigmoid()
).to("cuda:0")

# Define the loss function and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(MLPmodel.parameters(), lr=LEARNING_RATE)

# Train the model
def MLPmodel_func(X_train_tensor, y_train_tensor, X_test_tensor, y_test):
    for epoch in range(200):
        running_loss = 0.0
        batch_size = 1024
        for i in range(0, X_train_tensor.shape[0], batch_size):

            X_batch = X_train_tensor[i:i+batch_size]
            y_batch = y_train_tensor[i:i+batch_size]
            

            # Forward pass
            outputs = MLPmodel(X_batch).flatten()
            loss = criterion(outputs, y_batch)

            # Zero the parameter gradients
            optimizer.zero_grad()

            # Backward pass and optimization
            loss.backward()
            optimizer.step()

            # Accumulate the running loss
            running_loss += loss.item()

        # Compute the average loss for the epoch
        epoch_loss = running_loss / len(X_train_tensor)

    y_pred_test = MLPmodel(X_test_tensor).cpu().detach().numpy().flatten()
    recall = recall_score(y_true=y_test, y_pred=y_pred_test.round())
    accuracy = accuracy_score(y_true=y_test, y_pred=y_pred_test.round())
    precision = precision_score(y_true=y_test, y_pred=y_pred_test.round())
    f1 = f1_score(y_true=y_test, y_pred=y_pred_test.round())
    return accuracy,precision,recall,f1

def modelCreator (listType , dfs):
    back_counter = 2
    accuracy_list= []
    precision_list = []
    recall_list = []
    f_1_list = [] 
    for i in range(len(dfs)):
        data_model = {}
        concat_X = []
        concat_y = []
        concat_X.clear
        concat_y.clear
        X_main = dfs[i].drop(['Malware','HighestModDate'],axis= 1)
        y_main = dfs[i]['Malware']
        if i == 0 :
            X_train, X_test, y_train, y_test = train_test_split(X_main, y_main, test_size=0.3)
            X_main_tensor = torch.tensor(X_train.values, dtype=torch.float32).to("cuda:0")
            y_main_tensor = torch.tensor(y_train.values, dtype=torch.float32).to("cuda:0")
            X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to("cuda:0")
            y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to("cuda:0")
        elif i>0:
            for j in range(1,back_counter):
                X = dfs[i-j].drop(['Malware','HighestModDate'],axis= 1)
                y = dfs[i-j]['Malware']
                concat_X.append(X)
                concat_y.append(y)
            back_counter = back_counter + 1
            X_train = pd.concat(concat_X, ignore_index=True)
            y_train = pd.concat(concat_y,ignore_index=True)
            X_test = X_main
            y_test = y_main
            X_main_tensor = torch.tensor(X_train.values, dtype=torch.float32).to("cuda:0")
            y_main_tensor = torch.tensor(y_train.values, dtype=torch.float32).to("cuda:0")
            X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to("cuda:0")
            y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to("cuda:0")

        accuracy, precision, recall, f_1 = MLPmodel_func(X_main_tensor,y_main_tensor,X_test_tensor, y_test)
        
        accuracy_list.append(accuracy)
        precision_list.append(precision)
        recall_list.append(recall)
        f_1_list.append(f_1)
       

    return accuracy_list, precision_list,recall_list,f_1_list

dfstreeMonth = date_constraining(initial_date="2011-01-01", final_date="2018-12-31"
                                 , splitter='3MS',real_legitimate=real_legitimate
                                 ,real_malware=real_malware)
threeMonthAccuracy_list, threeMonthPrecision_list, threeMonthRecall_list, threeMonthF1_list = modelCreator(monthList3,dfstreeMonth)


avgAccThreeMonth = np.round(np.average(threeMonthAccuracy_list),2)
avgPreThreeMonth = np.round(np.average(threeMonthPrecision_list),2)
avgRecallThreeonth = np.round(np.average(threeMonthRecall_list),2)
avgF1ThreeMonth = np.round(np.average(threeMonthF1_list),2)

threeMonthComChart = (avgAccThreeMonth,avgPreThreeMonth,avgRecallThreeonth,avgF1ThreeMonth)

data_test_exp2 = date_constraining(initial_date="2018-12-01", final_date="2018-12-31"
                                    ,real_legitimate=real_legitimate,real_malware=real_malware)


data_train_exp1 = date_constraining(initial_date="2018-09-01", final_date="2018-12-01"
                                    ,real_legitimate=real_legitimate,real_malware=real_malware)


data_train_exp2  = date_constraining(initial_date="2018-06-01", final_date="2018-12-01"
                                    ,real_legitimate=real_legitimate,real_malware=real_malware)

data_train_exp3 = date_constraining(initial_date="2017-12-01", final_date="2018-12-01"
                                    ,real_legitimate=real_legitimate,real_malware=real_malware)


def modelCreator_exp (train_dfs , test_dfs):
    X_train = train_dfs.drop(['Malware','HighestModDate'],axis= 1)
    y_train = train_dfs['Malware']
    X_test = test_dfs.drop(['Malware','HighestModDate'],axis= 1)
    y_test = test_dfs['Malware']
    
    X_main_tensor = torch.tensor(X_train.values, dtype=torch.float32).to("cuda:0")
    y_main_tensor = torch.tensor(y_train.values, dtype=torch.float32).to("cuda:0")
    X_test_tensor = torch.tensor(X_test.values, dtype=torch.float32).to("cuda:0")
    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).to("cuda:0")
    accuracy, precision, recall, f_1 = MLPmodel_func(X_main_tensor,y_main_tensor,X_test_tensor, y_test)
    
    return accuracy, precision,recall,f_1

acc_3month, pre_3month, rec_3month, f_1_3month = modelCreator_exp(data_train_exp1,data_test_exp2)
acc_6month, pre_6month, rec_6month, f_1_6month = modelCreator_exp(data_train_exp2,data_test_exp2)
acc_12month, pre_12month, rec_12month, f_1_12month = modelCreator_exp(data_train_exp3,data_test_exp2)

three_last_MonthComChart = (acc_3month,pre_3month,rec_3month,f_1_3month)
six_last_MonthComChart = (acc_6month,pre_6month,rec_6month,f_1_6month)
twelve_last_MonthComChart = (acc_12month,pre_12month,rec_12month,f_1_12month)

with open("variables.txt", "w") as f:
    f.write(f"{threeMonthComChart}\n{three_last_MonthComChart}\n{six_last_MonthComChart}\n{twelve_last_MonthComChart}")